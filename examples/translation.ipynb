{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터셋: http://www.manythings.org/anki fra-eng\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pathlib\n",
    "import os\n",
    "\n",
    "sys.path.append(str(pathlib.Path(os.getcwd()).parent))\n",
    "\n",
    "import copy\n",
    "import re\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from src.base_module import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "print(f'device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 33000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def preprocess_sentence(sent):\n",
    "    sent = unicode_to_ascii(sent.lower())\n",
    "    sent = re.sub(r\"([?.!,¿])\", r\" \\1\", sent)\n",
    "    sent = re.sub(r\"[^a-zA-Z!.?]+\", r\" \", sent)\n",
    "    sent = re.sub(r\"\\s+\", \" \", sent)\n",
    "\n",
    "    return sent\n",
    "\n",
    "def load_preprocess_data():\n",
    "    encoder_input, decoder_input, decoder_target = [], [], []\n",
    "\n",
    "    with open('./fra.txt', 'r') as f:\n",
    "        lines = f.readlines()[:num_samples]\n",
    "        for i, line in enumerate(lines):\n",
    "            src, tar, _ = line.strip().split('\\t')\n",
    "            src = [w for w in preprocess_sentence(src).split()]\n",
    "            tar = preprocess_sentence(tar)\n",
    "            tgt_in = [w for w in f'<sos> {tar}'.split()]\n",
    "            tgt_out = [w for w in f'{tar} <eos>'.split()]\n",
    "\n",
    "            encoder_input.append(src)\n",
    "            decoder_input.append(tgt_in)\n",
    "            decoder_target.append(tgt_out)\n",
    "\n",
    "    return encoder_input, decoder_input, decoder_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Have you had dinner? -> have you had dinner ?\n",
      "Avez-vous déjà diné? -> avez vous deja dine ?\n"
     ]
    }
   ],
   "source": [
    "en_sent = u\"Have you had dinner?\"\n",
    "fr_sent = u\"Avez-vous déjà diné?\"\n",
    "\n",
    "print(f'{en_sent} -> {preprocess_sentence(en_sent)}')\n",
    "print(f'{fr_sent} -> {preprocess_sentence(fr_sent)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000\n",
      "33000\n",
      "33000\n"
     ]
    }
   ],
   "source": [
    "sents_en_in, sents_fra_in, sents_fra_out = load_preprocess_data()\n",
    "\n",
    "print(len(sents_en_in))\n",
    "print(len(sents_fra_in))\n",
    "print(len(sents_fra_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['go', '.'], ['go', '.'], ['go', '.'], ['go', '.'], ['hi', '.']]\n",
      "[['<sos>', 'va', '!'], ['<sos>', 'marche', '.'], ['<sos>', 'en', 'route', '!'], ['<sos>', 'bouge', '!'], ['<sos>', 'salut', '!']]\n",
      "[['va', '!', '<eos>'], ['marche', '.', '<eos>'], ['en', 'route', '!', '<eos>'], ['bouge', '!', '<eos>'], ['salut', '!', '<eos>']]\n"
     ]
    }
   ],
   "source": [
    "print(sents_en_in[:5])\n",
    "print(sents_fra_in[:5])\n",
    "print(sents_fra_out[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(sents):\n",
    "    words = []\n",
    "    for sent in sents:\n",
    "        for word in sent:\n",
    "            words.append(word)\n",
    "\n",
    "    word_counts = Counter(words)\n",
    "    vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "\n",
    "    word2index = {}\n",
    "    word2index['<PAD>'] = 0\n",
    "    word2index['<UNK>'] = 1\n",
    "\n",
    "    for i, word in enumerate(vocab):\n",
    "        word2index[word] = i + 2\n",
    "\n",
    "    return word2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src vocab size: 4486\n",
      "tar vocab size: 7879\n"
     ]
    }
   ],
   "source": [
    "src_vocab = build_vocab(sents_en_in)\n",
    "tgt_vocab = build_vocab(sents_fra_in + sents_fra_out)\n",
    "\n",
    "src_vocab_size = len(src_vocab)\n",
    "tgt_vocab_size = len(tgt_vocab)\n",
    "\n",
    "print(f'src vocab size: {src_vocab_size}')\n",
    "print(f'tar vocab size: {tgt_vocab_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "index2src = {v: k for k, v in src_vocab.items()}\n",
    "index2tar = {v: k for k, v in tgt_vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentences(sents, word2index):\n",
    "    encoded_data = []\n",
    "    for sent in tqdm(sents):\n",
    "        encoded_sent = []\n",
    "        for word in sent:\n",
    "            try:\n",
    "                encoded_sent.append(word2index[word])\n",
    "            except KeyError:\n",
    "                encoded_sent.append(word2index['<UNK>'])\n",
    "        encoded_data.append(encoded_sent)\n",
    "\n",
    "    return encoded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33000/33000 [00:00<00:00, 536445.40it/s]\n",
      "100%|██████████| 33000/33000 [00:00<00:00, 2197051.25it/s]\n",
      "100%|██████████| 33000/33000 [00:00<00:00, 2177659.41it/s]\n"
     ]
    }
   ],
   "source": [
    "encoder_input = encode_sentences(sents_en_in, src_vocab)\n",
    "decoder_input = encode_sentences(sents_fra_in, tgt_vocab)\n",
    "decoder_target = encode_sentences(sents_fra_out, tgt_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[27, 2], [27, 2], [27, 2], [27, 2], [736, 2]]\n",
      "[[3, 68, 11], [3, 204, 2], [3, 26, 491, 11], [3, 561, 11], [3, 954, 11]]\n",
      "[[68, 11, 4], [204, 2, 4], [26, 491, 11, 4], [561, 11, 4], [954, 11, 4]]\n"
     ]
    }
   ],
   "source": [
    "print(encoder_input[:5])\n",
    "print(decoder_input[:5])\n",
    "print(decoder_target[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentences(sents, max_len=None):\n",
    "    if max_len is None:\n",
    "        max_len = max([len(s) for s in sents])\n",
    "\n",
    "    features = np.zeros((len(sents), max_len), dtype=int)\n",
    "    for i, sent in enumerate(sents):\n",
    "        features[i, :len(sent)] = np.array(sent)[:max_len]\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = pad_sentences(encoder_input)\n",
    "decoder_input = pad_sentences(decoder_input)\n",
    "decoder_target = pad_sentences(decoder_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[27  2  0  0  0  0  0]\n",
      " [27  2  0  0  0  0  0]\n",
      " [27  2  0  0  0  0  0]]\n",
      "[[  3  68  11   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  3 204   2   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  3  26 491  11   0   0   0   0   0   0   0   0   0   0   0   0]]\n",
      "[[ 68  11   4   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [204   2   4   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [ 26 491  11   4   0   0   0   0   0   0   0   0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "print(encoder_input[:3])\n",
    "print(decoder_input[:3])\n",
    "print(decoder_target[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33000, 7)\n",
      "(33000, 16)\n",
      "(33000, 16)\n"
     ]
    }
   ],
   "source": [
    "print(encoder_input.shape)\n",
    "print(decoder_input.shape)\n",
    "print(decoder_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "랜덤 시퀀스 : [ 8594 15783 24996 ... 22517 20878 17362]\n"
     ]
    }
   ],
   "source": [
    "indices = np.arange(encoder_input.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "print('랜덤 시퀀스 :',indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = encoder_input[indices]\n",
    "decoder_input = decoder_input[indices]\n",
    "decoder_target = decoder_target[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'm', 'ticklish', '.', '<PAD>', '<PAD>', '<PAD>']\n",
      "['<sos>', 'je', 'suis', 'chatouilleuse', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "['je', 'suis', 'chatouilleuse', '.', '<eos>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n"
     ]
    }
   ],
   "source": [
    "print([index2src[word] for word in encoder_input[30997]])\n",
    "print([index2tar[word] for word in decoder_input[30997]])\n",
    "print([index2tar[word] for word in decoder_target[30997]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검증 데이터의 개수 : 3300\n"
     ]
    }
   ],
   "source": [
    "n_of_val = int(num_samples * 0.1)\n",
    "print('검증 데이터의 개수 :',n_of_val)\n",
    "\n",
    "encoder_input_train = encoder_input[:-n_of_val]\n",
    "decoder_input_train = decoder_input[:-n_of_val]\n",
    "decoder_target_train = decoder_target[:-n_of_val]\n",
    "\n",
    "encoder_input_test = encoder_input[-n_of_val:]\n",
    "decoder_input_test = decoder_input[-n_of_val:]\n",
    "decoder_target_test = decoder_target[-n_of_val:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 source 데이터의 크기 : (29700, 7)\n",
      "훈련 target 데이터의 크기 : (29700, 16)\n",
      "훈련 target 레이블의 크기 : (29700, 16)\n",
      "테스트 source 데이터의 크기 : (3300, 7)\n",
      "테스트 target 데이터의 크기 : (3300, 16)\n",
      "테스트 target 레이블의 크기 : (3300, 16)\n"
     ]
    }
   ],
   "source": [
    "print('훈련 source 데이터의 크기 :',encoder_input_train.shape)\n",
    "print('훈련 target 데이터의 크기 :',decoder_input_train.shape)\n",
    "print('훈련 target 레이블의 크기 :',decoder_target_train.shape)\n",
    "print('테스트 source 데이터의 크기 :',encoder_input_test.shape)\n",
    "print('테스트 target 데이터의 크기 :',decoder_input_test.shape)\n",
    "print('테스트 target 레이블의 크기 :',decoder_target_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_train_tensor = torch.tensor(encoder_input_train, dtype=torch.long)\n",
    "decoder_input_train_tensor = torch.tensor(decoder_input_train, dtype=torch.long)\n",
    "decoder_target_train_tensor = torch.tensor(decoder_target_train, dtype=torch.long)\n",
    "\n",
    "encoder_input_test_tensor = torch.tensor(encoder_input_test, dtype=torch.long)\n",
    "decoder_input_test_tensor = torch.tensor(decoder_input_test, dtype=torch.long)\n",
    "decoder_target_test_tensor = torch.tensor(decoder_target_test, dtype=torch.long)\n",
    "\n",
    "# 데이터셋 및 데이터로더 생성\n",
    "batch_size = 512\n",
    "\n",
    "train_dataset = TensorDataset(encoder_input_train_tensor, decoder_input_train_tensor, decoder_target_train_tensor)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "valid_dataset = TensorDataset(encoder_input_test_tensor, decoder_input_test_tensor, decoder_target_test_tensor)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 20\n",
    "d_embed = 128\n",
    "d_model = 128\n",
    "n_layer = 4\n",
    "h = 4\n",
    "d_ff = 256\n",
    "\n",
    "src_token_embed = TokenEmbedding(d_embed=d_embed, vocab_size=src_vocab_size)\n",
    "tgt_token_embed = TokenEmbedding(d_embed=d_embed, vocab_size=tgt_vocab_size)\n",
    "pos_embed = PositionalEncoding(d_embed=d_embed, max_len=max_len, device=device)\n",
    "src_embed = TransformerEmbedding(token_embed=src_token_embed, pos_embed=copy.deepcopy(pos_embed))\n",
    "tgt_embed = TransformerEmbedding(token_embed=tgt_token_embed, pos_embed=copy.deepcopy(pos_embed))\n",
    "\n",
    "attention = MultiHeadAttentionLayer(d_model=d_model, h=h, qkv_fc=nn.Linear(d_embed, d_model), out_fc=nn.Linear(d_model, d_embed)).to(device)\n",
    "position_ff = PositionWiseFeedForwardLayer(fc1=nn.Linear(d_embed, d_ff), fc2=nn.Linear(d_ff, d_embed)).to(device)\n",
    "\n",
    "encoder_block = EncoderBlock(self_attention=copy.deepcopy(attention), position_ff=copy.deepcopy(position_ff), d_model=d_model, device=device)\n",
    "decoder_block = DecoderBlock(self_attention=copy.deepcopy(attention), cross_attention=copy.deepcopy(attention), position_ff=copy.deepcopy(position_ff), d_model=d_model, device=device)\n",
    "\n",
    "encoder = Encoder(encoder_block=encoder_block, n_layer=n_layer)\n",
    "decoder = Decoder(decoder_block=decoder_block, n_layer=n_layer)\n",
    "\n",
    "generator = nn.Linear(d_embed, tgt_vocab_size)\n",
    "\n",
    "model = Transformer(\n",
    "    src_embed=src_embed,\n",
    "    tgt_embed=tgt_embed,\n",
    "    encoder=encoder,\n",
    "    decoder=decoder,\n",
    "    generator=generator\n",
    ").to(device)\n",
    "loss_function = nn.CrossEntropyLoss(ignore_index=0).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(model, dataloader, loss_function, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for encoder_inputs, decoder_inputs, decoder_targets in dataloader:\n",
    "            encoder_inputs = encoder_inputs.to(device)\n",
    "            decoder_inputs = decoder_inputs.to(device)\n",
    "            decoder_targets = decoder_targets.to(device)\n",
    "\n",
    "            outputs, _ = model(encoder_inputs, decoder_inputs)\n",
    "\n",
    "            loss = loss_function(outputs.view(-1, outputs.size(-1)), decoder_targets.view(-1))\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            mask = decoder_targets != 0\n",
    "            total_correct += ((outputs.argmax(dim=-1) == decoder_targets) * mask).sum().item()\n",
    "            total_count += mask.sum().item()\n",
    "\n",
    "    return total_loss / len(dataloader), total_correct / total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/59 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  3, 224,  16, 872,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0], device='mps:0')\n",
      "tensor([224,  16, 872,   2,   4,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0], device='mps:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for encoder_inputs, decoder_inputs, decoder_targets in tqdm(train_dataloader):\n",
    "    encoder_inputs = encoder_inputs.to(device)\n",
    "    decoder_inputs = decoder_inputs.to(device)\n",
    "    decoder_targets = decoder_targets.to(device)\n",
    "    print(decoder_inputs[0])\n",
    "    print(decoder_targets[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:07<00:00,  7.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/40 | Train Loss: 5.4994 | Train Acc: 0.4404 | Valid Loss: 5.9098 | Valid Acc: 0.4348\n",
      "Validation loss improved from inf to 5.9098. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:06<00:00,  8.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/40 | Train Loss: 3.6005 | Train Acc: 0.5078 | Valid Loss: 4.1833 | Valid Acc: 0.4924\n",
      "Validation loss improved from 5.9098 to 4.1833. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:06<00:00,  8.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/40 | Train Loss: 2.8520 | Train Acc: 0.5426 | Valid Loss: 3.5429 | Valid Acc: 0.5253\n",
      "Validation loss improved from 4.1833 to 3.5429. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:06<00:00,  8.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/40 | Train Loss: 2.4394 | Train Acc: 0.5641 | Valid Loss: 3.2080 | Valid Acc: 0.5441\n",
      "Validation loss improved from 3.5429 to 3.2080. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:07<00:00,  8.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/40 | Train Loss: 2.1807 | Train Acc: 0.5836 | Valid Loss: 2.9929 | Valid Acc: 0.5561\n",
      "Validation loss improved from 3.2080 to 2.9929. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:06<00:00,  8.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/40 | Train Loss: 1.9272 | Train Acc: 0.6086 | Valid Loss: 2.7914 | Valid Acc: 0.5813\n",
      "Validation loss improved from 2.9929 to 2.7914. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:07<00:00,  8.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/40 | Train Loss: 1.7340 | Train Acc: 0.6322 | Valid Loss: 2.6533 | Valid Acc: 0.5931\n",
      "Validation loss improved from 2.7914 to 2.6533. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:06<00:00,  8.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/40 | Train Loss: 1.6052 | Train Acc: 0.6451 | Valid Loss: 2.5651 | Valid Acc: 0.5988\n",
      "Validation loss improved from 2.6533 to 2.5651. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:06<00:00,  8.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/40 | Train Loss: 1.4422 | Train Acc: 0.6702 | Valid Loss: 2.4631 | Valid Acc: 0.6137\n",
      "Validation loss improved from 2.5651 to 2.4631. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:06<00:00,  8.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/40 | Train Loss: 1.3228 | Train Acc: 0.6889 | Valid Loss: 2.3813 | Valid Acc: 0.6237\n",
      "Validation loss improved from 2.4631 to 2.3813. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:06<00:00,  8.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11/40 | Train Loss: 1.2305 | Train Acc: 0.7034 | Valid Loss: 2.3249 | Valid Acc: 0.6326\n",
      "Validation loss improved from 2.3813 to 2.3249. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:06<00:00,  8.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12/40 | Train Loss: 1.1537 | Train Acc: 0.7144 | Valid Loss: 2.2847 | Valid Acc: 0.6350\n",
      "Validation loss improved from 2.3249 to 2.2847. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:06<00:00,  8.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13/40 | Train Loss: 1.0769 | Train Acc: 0.7283 | Valid Loss: 2.2481 | Valid Acc: 0.6391\n",
      "Validation loss improved from 2.2847 to 2.2481. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:06<00:00,  8.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14/40 | Train Loss: 1.0202 | Train Acc: 0.7390 | Valid Loss: 2.2084 | Valid Acc: 0.6460\n",
      "Validation loss improved from 2.2481 to 2.2084. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:06<00:00,  8.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15/40 | Train Loss: 0.9509 | Train Acc: 0.7488 | Valid Loss: 2.1860 | Valid Acc: 0.6549\n",
      "Validation loss improved from 2.2084 to 2.1860. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:06<00:00,  8.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16/40 | Train Loss: 0.9093 | Train Acc: 0.7577 | Valid Loss: 2.1524 | Valid Acc: 0.6559\n",
      "Validation loss improved from 2.1860 to 2.1524. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:06<00:00,  8.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17/40 | Train Loss: 0.8647 | Train Acc: 0.7677 | Valid Loss: 2.1313 | Valid Acc: 0.6638\n",
      "Validation loss improved from 2.1524 to 2.1313. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:06<00:00,  8.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18/40 | Train Loss: 0.8414 | Train Acc: 0.7712 | Valid Loss: 2.1280 | Valid Acc: 0.6657\n",
      "Validation loss improved from 2.1313 to 2.1280. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:06<00:00,  8.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19/40 | Train Loss: 0.8221 | Train Acc: 0.7754 | Valid Loss: 2.1303 | Valid Acc: 0.6652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:06<00:00,  8.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20/40 | Train Loss: 0.8011 | Train Acc: 0.7774 | Valid Loss: 2.1239 | Valid Acc: 0.6652\n",
      "Validation loss improved from 2.1280 to 2.1239. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:07<00:00,  8.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21/40 | Train Loss: 0.7678 | Train Acc: 0.7849 | Valid Loss: 2.1105 | Valid Acc: 0.6708\n",
      "Validation loss improved from 2.1239 to 2.1105. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:06<00:00,  8.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22/40 | Train Loss: 0.7377 | Train Acc: 0.7907 | Valid Loss: 2.0937 | Valid Acc: 0.6761\n",
      "Validation loss improved from 2.1105 to 2.0937. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:06<00:00,  8.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23/40 | Train Loss: 0.7188 | Train Acc: 0.7936 | Valid Loss: 2.0693 | Valid Acc: 0.6800\n",
      "Validation loss improved from 2.0937 to 2.0693. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:06<00:00,  8.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24/40 | Train Loss: 0.6986 | Train Acc: 0.7980 | Valid Loss: 2.0744 | Valid Acc: 0.6801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:07<00:00,  8.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25/40 | Train Loss: 0.6976 | Train Acc: 0.7976 | Valid Loss: 2.0935 | Valid Acc: 0.6785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:07<00:00,  8.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26/40 | Train Loss: 0.6792 | Train Acc: 0.8016 | Valid Loss: 2.0886 | Valid Acc: 0.6791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:06<00:00,  8.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27/40 | Train Loss: 0.6574 | Train Acc: 0.8047 | Valid Loss: 2.0742 | Valid Acc: 0.6814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:07<00:00,  8.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28/40 | Train Loss: 0.6428 | Train Acc: 0.8111 | Valid Loss: 2.0440 | Valid Acc: 0.6828\n",
      "Validation loss improved from 2.0693 to 2.0440. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:07<00:00,  8.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29/40 | Train Loss: 0.6219 | Train Acc: 0.8163 | Valid Loss: 2.0336 | Valid Acc: 0.6865\n",
      "Validation loss improved from 2.0440 to 2.0336. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:06<00:00,  8.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30/40 | Train Loss: 0.6171 | Train Acc: 0.8167 | Valid Loss: 2.0474 | Valid Acc: 0.6869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:07<00:00,  8.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31/40 | Train Loss: 0.6017 | Train Acc: 0.8205 | Valid Loss: 2.0378 | Valid Acc: 0.6859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:07<00:00,  8.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32/40 | Train Loss: 0.5875 | Train Acc: 0.8230 | Valid Loss: 2.0226 | Valid Acc: 0.6894\n",
      "Validation loss improved from 2.0336 to 2.0226. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:07<00:00,  8.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 33/40 | Train Loss: 0.5697 | Train Acc: 0.8278 | Valid Loss: 2.0281 | Valid Acc: 0.6895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:07<00:00,  8.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34/40 | Train Loss: 0.5510 | Train Acc: 0.8316 | Valid Loss: 2.0146 | Valid Acc: 0.6925\n",
      "Validation loss improved from 2.0226 to 2.0146. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:07<00:00,  8.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 35/40 | Train Loss: 0.5388 | Train Acc: 0.8339 | Valid Loss: 2.0174 | Valid Acc: 0.6954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:07<00:00,  8.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36/40 | Train Loss: 0.5361 | Train Acc: 0.8348 | Valid Loss: 2.0157 | Valid Acc: 0.6942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:07<00:00,  8.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 37/40 | Train Loss: 0.5264 | Train Acc: 0.8379 | Valid Loss: 2.0077 | Valid Acc: 0.6987\n",
      "Validation loss improved from 2.0146 to 2.0077. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:06<00:00,  8.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 38/40 | Train Loss: 0.5204 | Train Acc: 0.8387 | Valid Loss: 2.0147 | Valid Acc: 0.6953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:07<00:00,  8.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39/40 | Train Loss: 0.5316 | Train Acc: 0.8338 | Valid Loss: 2.0366 | Valid Acc: 0.6956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:07<00:00,  8.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 40/40 | Train Loss: 0.5223 | Train Acc: 0.8363 | Valid Loss: 2.0369 | Valid Acc: 0.6957\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    for encoder_inputs, decoder_inputs, decoder_targets in tqdm(train_dataloader):\n",
    "        encoder_inputs = encoder_inputs.to(device)\n",
    "        decoder_inputs = decoder_inputs.to(device)\n",
    "        decoder_targets = decoder_targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs, _ = model(encoder_inputs, decoder_inputs)\n",
    "\n",
    "        loss = loss_function(outputs.view(-1, outputs.size(-1)), decoder_targets.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss, train_acc = evaluation(model, train_dataloader, loss_function, device)\n",
    "    valid_loss, valid_acc = evaluation(model, valid_dataloader, loss_function, device)\n",
    "\n",
    "    print(f'Epoch: {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Valid Loss: {valid_loss:.4f} | Valid Acc: {valid_acc:.4f}')\n",
    "\n",
    "    if valid_loss < best_val_loss:\n",
    "        print(f'Validation loss improved from {best_val_loss:.4f} to {valid_loss:.4f}. 체크포인트를 저장합니다.')\n",
    "        best_val_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'best_model_checkpoint.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model validation loss: 2.0075\n",
      "Best model validation accuracy: 0.6981\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('best_model_checkpoint.pth', weights_only=True))\n",
    "model.to(device)\n",
    "\n",
    "val_loss, val_accuracy = evaluation(model, valid_dataloader, loss_function, device)\n",
    "\n",
    "print(f'Best model validation loss: {val_loss:.4f}')\n",
    "print(f'Best model validation accuracy: {val_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_to_src(input_seq):\n",
    "  sentence = ''\n",
    "  for encoded_word in input_seq:\n",
    "    if(encoded_word != 0):\n",
    "      sentence = sentence + index2src[encoded_word] + ' '\n",
    "  return sentence\n",
    "\n",
    "def seq_to_tar(input_seq):\n",
    "  sentence = ''\n",
    "  for encoded_word in input_seq:\n",
    "    if(encoded_word != 0 and encoded_word != tgt_vocab['<sos>'] and encoded_word != tgt_vocab['<eos>']):\n",
    "      sentence = sentence + index2tar[encoded_word] + ' '\n",
    "  return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq, model):\n",
    "    model.eval()\n",
    "    encoder_inputs = torch.LongTensor(input_seq).unsqueeze(0).to(device)\n",
    "    src_mask = model.make_src_mask(encoder_inputs)\n",
    "    encoder_out = model.encode(encoder_inputs, src_mask)\n",
    "\n",
    "    decoded_tokens = [tgt_vocab['<sos>']]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_len):\n",
    "            decoder_input = torch.LongTensor(decoded_tokens).unsqueeze(0).to(device)\n",
    "            tgt_mask = model.make_tgt_mask(decoder_input)\n",
    "            src_tgt_mask = model.make_src_tgt_mask(encoder_inputs, decoder_input)\n",
    "            output = model.decode(decoder_input, encoder_out, tgt_mask, src_tgt_mask)\n",
    "            output = model.generator(output)\n",
    "\n",
    "            output_token = output.argmax(dim=2)[:, -1].item()\n",
    "\n",
    "            if output_token == tgt_vocab['<eos>']:\n",
    "                break\n",
    "\n",
    "            decoded_tokens.append(output_token)\n",
    "\n",
    "    return ' '.join(index2tar[token] for token in decoded_tokens if token != tgt_vocab['<sos>'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력문장:  you weren t ready . \n",
      "정답문장:  vous n etiez pas pret . \n",
      "번역문장:  tu ne t aime pas pretes .\n",
      "--------------------------------------------------\n",
      "입력문장:  i saw you . \n",
      "정답문장:  je vous vis . \n",
      "번역문장:  je t ai vu .\n",
      "--------------------------------------------------\n",
      "입력문장:  i am a shy boy . \n",
      "정답문장:  je suis un garcon timide . \n",
      "번역문장:  je suis un garcon timide .\n",
      "--------------------------------------------------\n",
      "입력문장:  i loved you . \n",
      "정답문장:  je t aimais . \n",
      "번역문장:  je t aimais .\n",
      "--------------------------------------------------\n",
      "입력문장:  please help tom . \n",
      "정답문장:  aidez tom s il vous plait . \n",
      "번역문장:  aidez tom s il vous plait .\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for seq_index in [3, 50, 100, 300, 1001]:\n",
    "    input_seq = encoder_input_train[seq_index]\n",
    "    translated_text = decode_sequence(input_seq, model)\n",
    "\n",
    "    print(\"입력문장: \", seq_to_src(encoder_input_train[seq_index]))\n",
    "    print(\"정답문장: \", seq_to_tar(decoder_input_train[seq_index]))\n",
    "    print(\"번역문장: \", translated_text)\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
