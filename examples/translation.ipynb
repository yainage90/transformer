{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터셋: http://www.manythings.org/anki fra-eng\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pathlib\n",
    "import os\n",
    "\n",
    "sys.path.append(str(pathlib.Path(os.getcwd()).parent))\n",
    "\n",
    "import copy\n",
    "import re\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from src.base_module import *\n",
    "from src.transformer import Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "print(f'device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 33000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def preprocess_sentence(sent):\n",
    "    sent = unicode_to_ascii(sent.lower())\n",
    "    sent = re.sub(r\"([?.!,¿])\", r\" \\1\", sent)\n",
    "    sent = re.sub(r\"[^a-zA-Z!.?]+\", r\" \", sent)\n",
    "    sent = re.sub(r\"\\s+\", \" \", sent)\n",
    "\n",
    "    return sent\n",
    "\n",
    "def load_preprocess_data():\n",
    "    encoder_input, decoder_input, decoder_target = [], [], []\n",
    "\n",
    "    with open('./fra.txt', 'r') as f:\n",
    "        lines = f.readlines()[:num_samples]\n",
    "        for i, line in enumerate(lines):\n",
    "            src, tar, _ = line.strip().split('\\t')\n",
    "            src = [w for w in preprocess_sentence(src).split()]\n",
    "            tar = preprocess_sentence(tar)\n",
    "            tgt_in = [w for w in f'<sos> {tar}'.split()]\n",
    "            tgt_out = [w for w in f'{tar} <eos>'.split()]\n",
    "\n",
    "            encoder_input.append(src)\n",
    "            decoder_input.append(tgt_in)\n",
    "            decoder_target.append(tgt_out)\n",
    "\n",
    "    return encoder_input, decoder_input, decoder_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Have you had dinner? -> have you had dinner ?\n",
      "Avez-vous déjà diné? -> avez vous deja dine ?\n"
     ]
    }
   ],
   "source": [
    "en_sent = u\"Have you had dinner?\"\n",
    "fr_sent = u\"Avez-vous déjà diné?\"\n",
    "\n",
    "print(f'{en_sent} -> {preprocess_sentence(en_sent)}')\n",
    "print(f'{fr_sent} -> {preprocess_sentence(fr_sent)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000\n",
      "33000\n",
      "33000\n"
     ]
    }
   ],
   "source": [
    "sents_en_in, sents_fra_in, sents_fra_out = load_preprocess_data()\n",
    "\n",
    "print(len(sents_en_in))\n",
    "print(len(sents_fra_in))\n",
    "print(len(sents_fra_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['go', '.'], ['go', '.'], ['go', '.'], ['go', '.'], ['hi', '.']]\n",
      "[['<sos>', 'va', '!'], ['<sos>', 'marche', '.'], ['<sos>', 'en', 'route', '!'], ['<sos>', 'bouge', '!'], ['<sos>', 'salut', '!']]\n",
      "[['va', '!', '<eos>'], ['marche', '.', '<eos>'], ['en', 'route', '!', '<eos>'], ['bouge', '!', '<eos>'], ['salut', '!', '<eos>']]\n"
     ]
    }
   ],
   "source": [
    "print(sents_en_in[:5])\n",
    "print(sents_fra_in[:5])\n",
    "print(sents_fra_out[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(sents):\n",
    "    words = []\n",
    "    for sent in sents:\n",
    "        for word in sent:\n",
    "            words.append(word)\n",
    "\n",
    "    word_counts = Counter(words)\n",
    "    vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "\n",
    "    word2index = {}\n",
    "    word2index['<PAD>'] = 0\n",
    "    word2index['<UNK>'] = 1\n",
    "\n",
    "    for i, word in enumerate(vocab):\n",
    "        word2index[word] = i + 2\n",
    "\n",
    "    return word2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src vocab size: 4486\n",
      "tar vocab size: 7879\n"
     ]
    }
   ],
   "source": [
    "src_vocab = build_vocab(sents_en_in)\n",
    "tgt_vocab = build_vocab(sents_fra_in + sents_fra_out)\n",
    "\n",
    "src_vocab_size = len(src_vocab)\n",
    "tgt_vocab_size = len(tgt_vocab)\n",
    "\n",
    "print(f'src vocab size: {src_vocab_size}')\n",
    "print(f'tar vocab size: {tgt_vocab_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "index2src = {v: k for k, v in src_vocab.items()}\n",
    "index2tar = {v: k for k, v in tgt_vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentences(sents, word2index):\n",
    "    encoded_data = []\n",
    "    for sent in tqdm(sents):\n",
    "        encoded_sent = []\n",
    "        for word in sent:\n",
    "            try:\n",
    "                encoded_sent.append(word2index[word])\n",
    "            except KeyError:\n",
    "                encoded_sent.append(word2index['<UNK>'])\n",
    "        encoded_data.append(encoded_sent)\n",
    "\n",
    "    return encoded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33000/33000 [00:00<00:00, 526734.12it/s]\n",
      "100%|██████████| 33000/33000 [00:00<00:00, 2231840.17it/s]\n",
      "100%|██████████| 33000/33000 [00:00<00:00, 2252433.39it/s]\n"
     ]
    }
   ],
   "source": [
    "encoder_input = encode_sentences(sents_en_in, src_vocab)\n",
    "decoder_input = encode_sentences(sents_fra_in, tgt_vocab)\n",
    "decoder_target = encode_sentences(sents_fra_out, tgt_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[27, 2], [27, 2], [27, 2], [27, 2], [736, 2]]\n",
      "[[3, 68, 11], [3, 204, 2], [3, 26, 491, 11], [3, 561, 11], [3, 954, 11]]\n",
      "[[68, 11, 4], [204, 2, 4], [26, 491, 11, 4], [561, 11, 4], [954, 11, 4]]\n"
     ]
    }
   ],
   "source": [
    "print(encoder_input[:5])\n",
    "print(decoder_input[:5])\n",
    "print(decoder_target[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentences(sents, max_len=None):\n",
    "    if max_len is None:\n",
    "        max_len = max([len(s) for s in sents])\n",
    "\n",
    "    features = np.zeros((len(sents), max_len), dtype=int)\n",
    "    for i, sent in enumerate(sents):\n",
    "        features[i, :len(sent)] = np.array(sent)[:max_len]\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = pad_sentences(encoder_input)\n",
    "decoder_input = pad_sentences(decoder_input)\n",
    "decoder_target = pad_sentences(decoder_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[27  2  0  0  0  0  0]\n",
      " [27  2  0  0  0  0  0]\n",
      " [27  2  0  0  0  0  0]]\n",
      "[[  3  68  11   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  3 204   2   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  3  26 491  11   0   0   0   0   0   0   0   0   0   0   0   0]]\n",
      "[[ 68  11   4   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [204   2   4   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [ 26 491  11   4   0   0   0   0   0   0   0   0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "print(encoder_input[:3])\n",
    "print(decoder_input[:3])\n",
    "print(decoder_target[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33000, 7)\n",
      "(33000, 16)\n",
      "(33000, 16)\n"
     ]
    }
   ],
   "source": [
    "print(encoder_input.shape)\n",
    "print(decoder_input.shape)\n",
    "print(decoder_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "랜덤 시퀀스 : [16524 13883  6925 ... 16031 13706 11227]\n"
     ]
    }
   ],
   "source": [
    "indices = np.arange(encoder_input.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "print('랜덤 시퀀스 :',indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = encoder_input[indices]\n",
    "decoder_input = decoder_input[indices]\n",
    "decoder_target = decoder_target[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'can', 't', 'fix', 'this', '.', '<PAD>']\n",
      "['<sos>', 'je', 'n', 'arrive', 'pas', 'a', 'reparer', 'ceci', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "['je', 'n', 'arrive', 'pas', 'a', 'reparer', 'ceci', '.', '<eos>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n"
     ]
    }
   ],
   "source": [
    "print([index2src[word] for word in encoder_input[30997]])\n",
    "print([index2tar[word] for word in decoder_input[30997]])\n",
    "print([index2tar[word] for word in decoder_target[30997]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검증 데이터의 개수 : 3300\n"
     ]
    }
   ],
   "source": [
    "n_of_val = int(num_samples * 0.1)\n",
    "print('검증 데이터의 개수 :',n_of_val)\n",
    "\n",
    "encoder_input_train = encoder_input[:-n_of_val]\n",
    "decoder_input_train = decoder_input[:-n_of_val]\n",
    "decoder_target_train = decoder_target[:-n_of_val]\n",
    "\n",
    "encoder_input_test = encoder_input[-n_of_val:]\n",
    "decoder_input_test = decoder_input[-n_of_val:]\n",
    "decoder_target_test = decoder_target[-n_of_val:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 source 데이터의 크기 : (29700, 7)\n",
      "훈련 target 데이터의 크기 : (29700, 16)\n",
      "훈련 target 레이블의 크기 : (29700, 16)\n",
      "테스트 source 데이터의 크기 : (3300, 7)\n",
      "테스트 target 데이터의 크기 : (3300, 16)\n",
      "테스트 target 레이블의 크기 : (3300, 16)\n"
     ]
    }
   ],
   "source": [
    "print('훈련 source 데이터의 크기 :',encoder_input_train.shape)\n",
    "print('훈련 target 데이터의 크기 :',decoder_input_train.shape)\n",
    "print('훈련 target 레이블의 크기 :',decoder_target_train.shape)\n",
    "print('테스트 source 데이터의 크기 :',encoder_input_test.shape)\n",
    "print('테스트 target 데이터의 크기 :',decoder_input_test.shape)\n",
    "print('테스트 target 레이블의 크기 :',decoder_target_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_train_tensor = torch.tensor(encoder_input_train, dtype=torch.long)\n",
    "decoder_input_train_tensor = torch.tensor(decoder_input_train, dtype=torch.long)\n",
    "decoder_target_train_tensor = torch.tensor(decoder_target_train, dtype=torch.long)\n",
    "\n",
    "encoder_input_test_tensor = torch.tensor(encoder_input_test, dtype=torch.long)\n",
    "decoder_input_test_tensor = torch.tensor(decoder_input_test, dtype=torch.long)\n",
    "decoder_target_test_tensor = torch.tensor(decoder_target_test, dtype=torch.long)\n",
    "\n",
    "# 데이터셋 및 데이터로더 생성\n",
    "batch_size = 512\n",
    "\n",
    "train_dataset = TensorDataset(encoder_input_train_tensor, decoder_input_train_tensor, decoder_target_train_tensor)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "valid_dataset = TensorDataset(encoder_input_test_tensor, decoder_input_test_tensor, decoder_target_test_tensor)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 20\n",
    "d_embed = 128\n",
    "d_model = 128\n",
    "n_layer = 4\n",
    "h = 4\n",
    "d_ff = 256\n",
    "\n",
    "src_token_embed = TokenEmbedding(d_embed=d_embed, vocab_size=src_vocab_size)\n",
    "tgt_token_embed = TokenEmbedding(d_embed=d_embed, vocab_size=tgt_vocab_size)\n",
    "pos_embed = PositionalEncoding(d_embed=d_embed, max_len=max_len, device=device)\n",
    "src_embed = TransformerEmbedding(token_embed=src_token_embed, pos_embed=copy.deepcopy(pos_embed))\n",
    "tgt_embed = TransformerEmbedding(token_embed=tgt_token_embed, pos_embed=copy.deepcopy(pos_embed))\n",
    "\n",
    "attention = MultiHeadAttentionLayer(d_model=d_model, h=h, qkv_fc=nn.Linear(d_embed, d_model), out_fc=nn.Linear(d_model, d_embed)).to(device)\n",
    "position_ff = PositionWiseFeedForwardLayer(fc1=nn.Linear(d_embed, d_ff), fc2=nn.Linear(d_ff, d_embed)).to(device)\n",
    "\n",
    "encoder_block = EncoderBlock(self_attention=copy.deepcopy(attention), position_ff=copy.deepcopy(position_ff), d_model=d_model, device=device)\n",
    "decoder_block = DecoderBlock(self_attention=copy.deepcopy(attention), cross_attention=copy.deepcopy(attention), position_ff=copy.deepcopy(position_ff), d_model=d_model, device=device)\n",
    "\n",
    "encoder = Encoder(encoder_block=encoder_block, n_layer=n_layer)\n",
    "decoder = Decoder(decoder_block=decoder_block, n_layer=n_layer)\n",
    "\n",
    "generator = nn.Linear(d_embed, tgt_vocab_size)\n",
    "\n",
    "model = Transformer(\n",
    "    src_embed=src_embed,\n",
    "    tgt_embed=tgt_embed,\n",
    "    encoder=encoder,\n",
    "    decoder=decoder,\n",
    "    generator=generator\n",
    ").to(device)\n",
    "loss_function = nn.CrossEntropyLoss(ignore_index=0).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(model, dataloader, loss_function, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for encoder_inputs, decoder_inputs, decoder_targets in dataloader:\n",
    "            encoder_inputs = encoder_inputs.to(device)\n",
    "            decoder_inputs = decoder_inputs.to(device)\n",
    "            decoder_targets = decoder_targets.to(device)\n",
    "\n",
    "            outputs, _ = model(encoder_inputs, decoder_inputs)\n",
    "\n",
    "            loss = loss_function(outputs.view(-1, outputs.size(-1)), decoder_targets.view(-1))\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            mask = decoder_targets != 0\n",
    "            total_correct += ((outputs.argmax(dim=-1) == decoder_targets) * mask).sum().item()\n",
    "            total_count += mask.sum().item()\n",
    "\n",
    "    return total_loss / len(dataloader), total_correct / total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/59 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  3,   8,   6, 400, 531,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0], device='mps:0')\n",
      "tensor([  8,   6, 400, 531,   2,   4,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0], device='mps:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for encoder_inputs, decoder_inputs, decoder_targets in tqdm(train_dataloader):\n",
    "    encoder_inputs = encoder_inputs.to(device)\n",
    "    decoder_inputs = decoder_inputs.to(device)\n",
    "    decoder_targets = decoder_targets.to(device)\n",
    "    print(decoder_inputs[0])\n",
    "    print(decoder_targets[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:08<00:00,  7.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/40 | Train Loss: 5.2724 | Train Acc: 0.4410 | Valid Loss: 5.8602 | Valid Acc: 0.4267\n",
      "Validation loss improved from inf to 5.8602. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:07<00:00,  8.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/40 | Train Loss: 3.4791 | Train Acc: 0.5147 | Valid Loss: 4.1664 | Valid Acc: 0.4946\n",
      "Validation loss improved from 5.8602 to 4.1664. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:06<00:00,  8.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/40 | Train Loss: 2.7174 | Train Acc: 0.5526 | Valid Loss: 3.4928 | Valid Acc: 0.5282\n",
      "Validation loss improved from 4.1664 to 3.4928. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:07<00:00,  8.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/40 | Train Loss: 2.3150 | Train Acc: 0.5780 | Valid Loss: 3.1393 | Valid Acc: 0.5467\n",
      "Validation loss improved from 3.4928 to 3.1393. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:06<00:00,  8.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/40 | Train Loss: 2.0305 | Train Acc: 0.6064 | Valid Loss: 2.9188 | Valid Acc: 0.5658\n",
      "Validation loss improved from 3.1393 to 2.9188. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:07<00:00,  8.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/40 | Train Loss: 1.7921 | Train Acc: 0.6295 | Valid Loss: 2.7552 | Valid Acc: 0.5809\n",
      "Validation loss improved from 2.9188 to 2.7552. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:07<00:00,  8.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/40 | Train Loss: 1.6317 | Train Acc: 0.6489 | Valid Loss: 2.6365 | Valid Acc: 0.5942\n",
      "Validation loss improved from 2.7552 to 2.6365. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:06<00:00,  8.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/40 | Train Loss: 1.4825 | Train Acc: 0.6684 | Valid Loss: 2.5388 | Valid Acc: 0.6043\n",
      "Validation loss improved from 2.6365 to 2.5388. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:07<00:00,  8.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/40 | Train Loss: 1.3462 | Train Acc: 0.6878 | Valid Loss: 2.4629 | Valid Acc: 0.6146\n",
      "Validation loss improved from 2.5388 to 2.4629. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:07<00:00,  8.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/40 | Train Loss: 1.2280 | Train Acc: 0.7021 | Valid Loss: 2.4185 | Valid Acc: 0.6183\n",
      "Validation loss improved from 2.4629 to 2.4185. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:06<00:00,  8.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11/40 | Train Loss: 1.1485 | Train Acc: 0.7172 | Valid Loss: 2.3628 | Valid Acc: 0.6269\n",
      "Validation loss improved from 2.4185 to 2.3628. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:06<00:00,  8.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12/40 | Train Loss: 1.0602 | Train Acc: 0.7301 | Valid Loss: 2.3103 | Valid Acc: 0.6328\n",
      "Validation loss improved from 2.3628 to 2.3103. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:06<00:00,  8.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13/40 | Train Loss: 0.9944 | Train Acc: 0.7456 | Valid Loss: 2.2896 | Valid Acc: 0.6391\n",
      "Validation loss improved from 2.3103 to 2.2896. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:06<00:00,  8.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14/40 | Train Loss: 0.9371 | Train Acc: 0.7523 | Valid Loss: 2.2636 | Valid Acc: 0.6467\n",
      "Validation loss improved from 2.2896 to 2.2636. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:06<00:00,  8.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15/40 | Train Loss: 0.8999 | Train Acc: 0.7594 | Valid Loss: 2.2625 | Valid Acc: 0.6489\n",
      "Validation loss improved from 2.2636 to 2.2625. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:06<00:00,  8.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16/40 | Train Loss: 0.8583 | Train Acc: 0.7680 | Valid Loss: 2.2252 | Valid Acc: 0.6496\n",
      "Validation loss improved from 2.2625 to 2.2252. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:07<00:00,  8.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17/40 | Train Loss: 0.8360 | Train Acc: 0.7712 | Valid Loss: 2.2293 | Valid Acc: 0.6523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:07<00:00,  8.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18/40 | Train Loss: 0.7927 | Train Acc: 0.7799 | Valid Loss: 2.1880 | Valid Acc: 0.6606\n",
      "Validation loss improved from 2.2252 to 2.1880. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:06<00:00,  8.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19/40 | Train Loss: 0.7697 | Train Acc: 0.7849 | Valid Loss: 2.2024 | Valid Acc: 0.6615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:07<00:00,  8.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20/40 | Train Loss: 0.7490 | Train Acc: 0.7843 | Valid Loss: 2.1989 | Valid Acc: 0.6612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:07<00:00,  8.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21/40 | Train Loss: 0.7261 | Train Acc: 0.7934 | Valid Loss: 2.1653 | Valid Acc: 0.6649\n",
      "Validation loss improved from 2.1880 to 2.1653. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:07<00:00,  8.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22/40 | Train Loss: 0.6943 | Train Acc: 0.8011 | Valid Loss: 2.1633 | Valid Acc: 0.6704\n",
      "Validation loss improved from 2.1653 to 2.1633. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:07<00:00,  8.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23/40 | Train Loss: 0.6844 | Train Acc: 0.8036 | Valid Loss: 2.1588 | Valid Acc: 0.6718\n",
      "Validation loss improved from 2.1633 to 2.1588. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:07<00:00,  8.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24/40 | Train Loss: 0.6639 | Train Acc: 0.8067 | Valid Loss: 2.1574 | Valid Acc: 0.6698\n",
      "Validation loss improved from 2.1588 to 2.1574. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:06<00:00,  8.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25/40 | Train Loss: 0.6478 | Train Acc: 0.8090 | Valid Loss: 2.1456 | Valid Acc: 0.6777\n",
      "Validation loss improved from 2.1574 to 2.1456. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:07<00:00,  8.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26/40 | Train Loss: 0.6228 | Train Acc: 0.8136 | Valid Loss: 2.1480 | Valid Acc: 0.6723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:07<00:00,  8.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27/40 | Train Loss: 0.6174 | Train Acc: 0.8161 | Valid Loss: 2.1334 | Valid Acc: 0.6770\n",
      "Validation loss improved from 2.1456 to 2.1334. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:07<00:00,  8.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28/40 | Train Loss: 0.6075 | Train Acc: 0.8169 | Valid Loss: 2.1382 | Valid Acc: 0.6754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:07<00:00,  8.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29/40 | Train Loss: 0.5954 | Train Acc: 0.8222 | Valid Loss: 2.1527 | Valid Acc: 0.6789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:07<00:00,  8.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30/40 | Train Loss: 0.6023 | Train Acc: 0.8185 | Valid Loss: 2.1750 | Valid Acc: 0.6740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:06<00:00,  8.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31/40 | Train Loss: 0.6022 | Train Acc: 0.8210 | Valid Loss: 2.1620 | Valid Acc: 0.6781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:07<00:00,  8.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32/40 | Train Loss: 0.5716 | Train Acc: 0.8275 | Valid Loss: 2.1427 | Valid Acc: 0.6816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:07<00:00,  8.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 33/40 | Train Loss: 0.5671 | Train Acc: 0.8260 | Valid Loss: 2.1740 | Valid Acc: 0.6826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:07<00:00,  8.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34/40 | Train Loss: 0.5654 | Train Acc: 0.8294 | Valid Loss: 2.1371 | Valid Acc: 0.6824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:07<00:00,  8.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 35/40 | Train Loss: 0.5469 | Train Acc: 0.8310 | Valid Loss: 2.1385 | Valid Acc: 0.6840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:06<00:00,  8.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36/40 | Train Loss: 0.5309 | Train Acc: 0.8360 | Valid Loss: 2.1240 | Valid Acc: 0.6869\n",
      "Validation loss improved from 2.1334 to 2.1240. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:07<00:00,  8.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 37/40 | Train Loss: 0.5191 | Train Acc: 0.8378 | Valid Loss: 2.1389 | Valid Acc: 0.6890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:07<00:00,  8.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 38/40 | Train Loss: 0.5041 | Train Acc: 0.8410 | Valid Loss: 2.1216 | Valid Acc: 0.6912\n",
      "Validation loss improved from 2.1240 to 2.1216. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:07<00:00,  8.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39/40 | Train Loss: 0.4921 | Train Acc: 0.8456 | Valid Loss: 2.1129 | Valid Acc: 0.6920\n",
      "Validation loss improved from 2.1216 to 2.1129. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:07<00:00,  8.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 40/40 | Train Loss: 0.4846 | Train Acc: 0.8484 | Valid Loss: 2.1095 | Valid Acc: 0.6942\n",
      "Validation loss improved from 2.1129 to 2.1095. 체크포인트를 저장합니다.\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    for encoder_inputs, decoder_inputs, decoder_targets in tqdm(train_dataloader):\n",
    "        encoder_inputs = encoder_inputs.to(device)\n",
    "        decoder_inputs = decoder_inputs.to(device)\n",
    "        decoder_targets = decoder_targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs, _ = model(encoder_inputs, decoder_inputs)\n",
    "\n",
    "        loss = loss_function(outputs.view(-1, outputs.size(-1)), decoder_targets.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss, train_acc = evaluation(model, train_dataloader, loss_function, device)\n",
    "    valid_loss, valid_acc = evaluation(model, valid_dataloader, loss_function, device)\n",
    "\n",
    "    print(f'Epoch: {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Valid Loss: {valid_loss:.4f} | Valid Acc: {valid_acc:.4f}')\n",
    "\n",
    "    if valid_loss < best_val_loss:\n",
    "        print(f'Validation loss improved from {best_val_loss:.4f} to {valid_loss:.4f}. 체크포인트를 저장합니다.')\n",
    "        best_val_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'best_model_checkpoint.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model validation loss: 2.1102\n",
      "Best model validation accuracy: 0.6925\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('best_model_checkpoint.pth', weights_only=True))\n",
    "model.to(device)\n",
    "\n",
    "val_loss, val_accuracy = evaluation(model, valid_dataloader, loss_function, device)\n",
    "\n",
    "print(f'Best model validation loss: {val_loss:.4f}')\n",
    "print(f'Best model validation accuracy: {val_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_to_src(input_seq):\n",
    "  sentence = ''\n",
    "  for encoded_word in input_seq:\n",
    "    if(encoded_word != 0):\n",
    "      sentence = sentence + index2src[encoded_word] + ' '\n",
    "  return sentence\n",
    "\n",
    "def seq_to_tar(input_seq):\n",
    "  sentence = ''\n",
    "  for encoded_word in input_seq:\n",
    "    if(encoded_word != 0 and encoded_word != tgt_vocab['<sos>'] and encoded_word != tgt_vocab['<eos>']):\n",
    "      sentence = sentence + index2tar[encoded_word] + ' '\n",
    "  return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq, model):\n",
    "    model.eval()\n",
    "    encoder_inputs = torch.LongTensor(input_seq).unsqueeze(0).to(device)\n",
    "    src_mask = model.make_src_mask(encoder_inputs)\n",
    "    encoder_out = model.encode(encoder_inputs, src_mask)\n",
    "\n",
    "    decoded_tokens = [tgt_vocab['<sos>']]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_len):\n",
    "            decoder_input = torch.LongTensor(decoded_tokens).unsqueeze(0).to(device)\n",
    "            tgt_mask = model.make_tgt_mask(decoder_input)\n",
    "            src_tgt_mask = model.make_src_tgt_mask(encoder_inputs, decoder_input)\n",
    "            output = model.decode(decoder_input, encoder_out, tgt_mask, src_tgt_mask)\n",
    "            output = model.generator(output)\n",
    "\n",
    "            output_token = output.argmax(dim=2)[:, -1].item()\n",
    "\n",
    "            if output_token == tgt_vocab['<eos>']:\n",
    "                break\n",
    "\n",
    "            decoded_tokens.append(output_token)\n",
    "\n",
    "    return ' '.join(index2tar[token] for token in decoded_tokens if token != tgt_vocab['<sos>'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력문장:  where s the beach ? \n",
      "정답문장:  ou est la plage ? \n",
      "번역문장:  ou sont les cheveux ?\n",
      "--------------------------------------------------\n",
      "입력문장:  tom won t retire . \n",
      "정답문장:  tom ne prendra pas sa retraite . \n",
      "번역문장:  tom ne prendra pas en francais .\n",
      "--------------------------------------------------\n",
      "입력문장:  tom is my nephew . \n",
      "정답문장:  tom est mon neveu . \n",
      "번역문장:  tom est mon neveu .\n",
      "--------------------------------------------------\n",
      "입력문장:  be confident . \n",
      "정답문장:  sois confiant ! \n",
      "번역문장:  soyez confiants !\n",
      "--------------------------------------------------\n",
      "입력문장:  don t tell anybody . \n",
      "정답문장:  ne le dis a quiconque ! \n",
      "번역문장:  ne le dis a personne !\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for seq_index in [3, 50, 100, 300, 1001]:\n",
    "    input_seq = encoder_input_train[seq_index]\n",
    "    translated_text = decode_sequence(input_seq, model)\n",
    "\n",
    "    print(\"입력문장: \", seq_to_src(encoder_input_train[seq_index]))\n",
    "    print(\"정답문장: \", seq_to_tar(decoder_input_train[seq_index]))\n",
    "    print(\"번역문장: \", translated_text)\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
