{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터셋: http://www.manythings.org/anki fra-eng\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pathlib\n",
    "import os\n",
    "\n",
    "sys.path.append(str(pathlib.Path(os.getcwd()).parent))\n",
    "\n",
    "import copy\n",
    "import re\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from src.base_module import *\n",
    "from src.transformer import Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "print(f'device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 33000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def preprocess_sentence(sent):\n",
    "    sent = unicode_to_ascii(sent.lower())\n",
    "    sent = re.sub(r\"([?.!,¿])\", r\" \\1\", sent)\n",
    "    sent = re.sub(r\"[^a-zA-Z!.?]+\", r\" \", sent)\n",
    "    sent = re.sub(r\"\\s+\", \" \", sent)\n",
    "\n",
    "    return sent\n",
    "\n",
    "def load_preprocess_data():\n",
    "    encoder_input, decoder_input, decoder_target = [], [], []\n",
    "\n",
    "    with open('./fra.txt', 'r') as f:\n",
    "        lines = f.readlines()[:num_samples]\n",
    "        for i, line in enumerate(lines):\n",
    "            src, tar, _ = line.strip().split('\\t')\n",
    "            src = [w for w in preprocess_sentence(src).split()]\n",
    "            tar = preprocess_sentence(tar)\n",
    "            tgt_in = [w for w in f'<sos> {tar}'.split()]\n",
    "            tgt_out = [w for w in f'{tar} <eos>'.split()]\n",
    "\n",
    "            encoder_input.append(src)\n",
    "            decoder_input.append(tgt_in)\n",
    "            decoder_target.append(tgt_out)\n",
    "\n",
    "    return encoder_input, decoder_input, decoder_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Have you had dinner? -> have you had dinner ?\n",
      "Avez-vous déjà diné? -> avez vous deja dine ?\n"
     ]
    }
   ],
   "source": [
    "en_sent = u\"Have you had dinner?\"\n",
    "fr_sent = u\"Avez-vous déjà diné?\"\n",
    "\n",
    "print(f'{en_sent} -> {preprocess_sentence(en_sent)}')\n",
    "print(f'{fr_sent} -> {preprocess_sentence(fr_sent)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000\n",
      "33000\n",
      "33000\n"
     ]
    }
   ],
   "source": [
    "sents_en_in, sents_fra_in, sents_fra_out = load_preprocess_data()\n",
    "\n",
    "print(len(sents_en_in))\n",
    "print(len(sents_fra_in))\n",
    "print(len(sents_fra_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['go', '.'], ['go', '.'], ['go', '.'], ['go', '.'], ['hi', '.']]\n",
      "[['<sos>', 'va', '!'], ['<sos>', 'marche', '.'], ['<sos>', 'en', 'route', '!'], ['<sos>', 'bouge', '!'], ['<sos>', 'salut', '!']]\n",
      "[['va', '!', '<eos>'], ['marche', '.', '<eos>'], ['en', 'route', '!', '<eos>'], ['bouge', '!', '<eos>'], ['salut', '!', '<eos>']]\n"
     ]
    }
   ],
   "source": [
    "print(sents_en_in[:5])\n",
    "print(sents_fra_in[:5])\n",
    "print(sents_fra_out[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(sents):\n",
    "    words = []\n",
    "    for sent in sents:\n",
    "        for word in sent:\n",
    "            words.append(word)\n",
    "\n",
    "    word_counts = Counter(words)\n",
    "    vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "\n",
    "    word2index = {}\n",
    "    word2index['<PAD>'] = 0\n",
    "    word2index['<UNK>'] = 1\n",
    "\n",
    "    for i, word in enumerate(vocab):\n",
    "        word2index[word] = i + 2\n",
    "\n",
    "    return word2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src vocab size: 4486\n",
      "tar vocab size: 7879\n"
     ]
    }
   ],
   "source": [
    "src_vocab = build_vocab(sents_en_in)\n",
    "tgt_vocab = build_vocab(sents_fra_in + sents_fra_out)\n",
    "\n",
    "src_vocab_size = len(src_vocab)\n",
    "tgt_vocab_size = len(tgt_vocab)\n",
    "\n",
    "print(f'src vocab size: {src_vocab_size}')\n",
    "print(f'tar vocab size: {tgt_vocab_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "index2src = {v: k for k, v in src_vocab.items()}\n",
    "index2tar = {v: k for k, v in tgt_vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentences(sents, word2index):\n",
    "    encoded_data = []\n",
    "    for sent in tqdm(sents):\n",
    "        encoded_sent = []\n",
    "        for word in sent:\n",
    "            try:\n",
    "                encoded_sent.append(word2index[word])\n",
    "            except KeyError:\n",
    "                encoded_sent.append(word2index['<UNK>'])\n",
    "        encoded_data.append(encoded_sent)\n",
    "\n",
    "    return encoded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33000/33000 [00:00<00:00, 537589.26it/s]\n",
      "100%|██████████| 33000/33000 [00:00<00:00, 2220597.01it/s]\n",
      "100%|██████████| 33000/33000 [00:00<00:00, 2283273.38it/s]\n"
     ]
    }
   ],
   "source": [
    "encoder_input = encode_sentences(sents_en_in, src_vocab)\n",
    "decoder_input = encode_sentences(sents_fra_in, tgt_vocab)\n",
    "decoder_target = encode_sentences(sents_fra_out, tgt_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[27, 2], [27, 2], [27, 2], [27, 2], [736, 2]]\n",
      "[[3, 68, 11], [3, 204, 2], [3, 26, 491, 11], [3, 561, 11], [3, 954, 11]]\n",
      "[[68, 11, 4], [204, 2, 4], [26, 491, 11, 4], [561, 11, 4], [954, 11, 4]]\n"
     ]
    }
   ],
   "source": [
    "print(encoder_input[:5])\n",
    "print(decoder_input[:5])\n",
    "print(decoder_target[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentences(sents, max_len=None):\n",
    "    if max_len is None:\n",
    "        max_len = max([len(s) for s in sents])\n",
    "\n",
    "    features = np.zeros((len(sents), max_len), dtype=int)\n",
    "    for i, sent in enumerate(sents):\n",
    "        features[i, :len(sent)] = np.array(sent)[:max_len]\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = pad_sentences(encoder_input)\n",
    "decoder_input = pad_sentences(decoder_input)\n",
    "decoder_target = pad_sentences(decoder_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[27  2  0  0  0  0  0]\n",
      " [27  2  0  0  0  0  0]\n",
      " [27  2  0  0  0  0  0]]\n",
      "[[  3  68  11   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  3 204   2   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  3  26 491  11   0   0   0   0   0   0   0   0   0   0   0   0]]\n",
      "[[ 68  11   4   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [204   2   4   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [ 26 491  11   4   0   0   0   0   0   0   0   0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "print(encoder_input[:3])\n",
    "print(decoder_input[:3])\n",
    "print(decoder_target[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33000, 7)\n",
      "(33000, 16)\n",
      "(33000, 16)\n"
     ]
    }
   ],
   "source": [
    "print(encoder_input.shape)\n",
    "print(decoder_input.shape)\n",
    "print(decoder_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "랜덤 시퀀스 : [30083 30064  7979 ...  4385  3419  5667]\n"
     ]
    }
   ],
   "source": [
    "indices = np.arange(encoder_input.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "print('랜덤 시퀀스 :',indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = encoder_input[indices]\n",
    "decoder_input = decoder_input[indices]\n",
    "decoder_target = decoder_target[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['slow', 'down', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "['<sos>', 'calmos', '!', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "['calmos', '!', '<eos>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n"
     ]
    }
   ],
   "source": [
    "print([index2src[word] for word in encoder_input[30997]])\n",
    "print([index2tar[word] for word in decoder_input[30997]])\n",
    "print([index2tar[word] for word in decoder_target[30997]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검증 데이터의 개수 : 3300\n"
     ]
    }
   ],
   "source": [
    "n_of_val = int(num_samples * 0.1)\n",
    "print('검증 데이터의 개수 :',n_of_val)\n",
    "\n",
    "encoder_input_train = encoder_input[:-n_of_val]\n",
    "decoder_input_train = decoder_input[:-n_of_val]\n",
    "decoder_target_train = decoder_target[:-n_of_val]\n",
    "\n",
    "encoder_input_test = encoder_input[-n_of_val:]\n",
    "decoder_input_test = decoder_input[-n_of_val:]\n",
    "decoder_target_test = decoder_target[-n_of_val:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 source 데이터의 크기 : (29700, 7)\n",
      "훈련 target 데이터의 크기 : (29700, 16)\n",
      "훈련 target 레이블의 크기 : (29700, 16)\n",
      "테스트 source 데이터의 크기 : (3300, 7)\n",
      "테스트 target 데이터의 크기 : (3300, 16)\n",
      "테스트 target 레이블의 크기 : (3300, 16)\n"
     ]
    }
   ],
   "source": [
    "print('훈련 source 데이터의 크기 :',encoder_input_train.shape)\n",
    "print('훈련 target 데이터의 크기 :',decoder_input_train.shape)\n",
    "print('훈련 target 레이블의 크기 :',decoder_target_train.shape)\n",
    "print('테스트 source 데이터의 크기 :',encoder_input_test.shape)\n",
    "print('테스트 target 데이터의 크기 :',decoder_input_test.shape)\n",
    "print('테스트 target 레이블의 크기 :',decoder_target_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_train_tensor = torch.tensor(encoder_input_train, dtype=torch.long)\n",
    "decoder_input_train_tensor = torch.tensor(decoder_input_train, dtype=torch.long)\n",
    "decoder_target_train_tensor = torch.tensor(decoder_target_train, dtype=torch.long)\n",
    "\n",
    "encoder_input_test_tensor = torch.tensor(encoder_input_test, dtype=torch.long)\n",
    "decoder_input_test_tensor = torch.tensor(decoder_input_test, dtype=torch.long)\n",
    "decoder_target_test_tensor = torch.tensor(decoder_target_test, dtype=torch.long)\n",
    "\n",
    "# 데이터셋 및 데이터로더 생성\n",
    "batch_size = 512\n",
    "\n",
    "train_dataset = TensorDataset(encoder_input_train_tensor, decoder_input_train_tensor, decoder_target_train_tensor)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "valid_dataset = TensorDataset(encoder_input_test_tensor, decoder_input_test_tensor, decoder_target_test_tensor)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 20\n",
    "d_embed = 128\n",
    "d_model = 128\n",
    "n_layer = 4\n",
    "h = 4\n",
    "d_ff = 256\n",
    "\n",
    "src_token_embed = TokenEmbedding(d_embed=d_embed, vocab_size=src_vocab_size)\n",
    "tgt_token_embed = TokenEmbedding(d_embed=d_embed, vocab_size=tgt_vocab_size)\n",
    "pos_embed = PositionalEncoding(d_embed=d_embed, max_len=max_len)\n",
    "src_embed = TransformerEmbedding(token_embed=src_token_embed, pos_embed=copy.deepcopy(pos_embed))\n",
    "tgt_embed = TransformerEmbedding(token_embed=tgt_token_embed, pos_embed=copy.deepcopy(pos_embed))\n",
    "\n",
    "attention = MultiHeadAttentionLayer(d_model=d_model, h=h, qkv_fc=nn.Linear(d_embed, d_model), out_fc=nn.Linear(d_model, d_embed))\n",
    "position_ff = PositionWiseFeedForwardLayer(fc1=nn.Linear(d_embed, d_ff), fc2=nn.Linear(d_ff, d_embed))\n",
    "\n",
    "encoder_block = EncoderBlock(self_attention=copy.deepcopy(attention), position_ff=copy.deepcopy(position_ff), d_model=d_model)\n",
    "decoder_block = DecoderBlock(self_attention=copy.deepcopy(attention), cross_attention=copy.deepcopy(attention), position_ff=copy.deepcopy(position_ff), d_model=d_model)\n",
    "\n",
    "encoder = Encoder(encoder_block=encoder_block, n_layer=n_layer)\n",
    "decoder = Decoder(decoder_block=decoder_block, n_layer=n_layer)\n",
    "\n",
    "generator = nn.Linear(d_embed, tgt_vocab_size)\n",
    "\n",
    "model = Transformer(\n",
    "    src_embed=src_embed,\n",
    "    tgt_embed=tgt_embed,\n",
    "    encoder=encoder,\n",
    "    decoder=decoder,\n",
    "    generator=generator\n",
    ").to(device)\n",
    "loss_function = nn.CrossEntropyLoss(ignore_index=0).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(model, dataloader, loss_function, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for encoder_inputs, decoder_inputs, decoder_targets in dataloader:\n",
    "            encoder_inputs = encoder_inputs.to(device)\n",
    "            decoder_inputs = decoder_inputs.to(device)\n",
    "            decoder_targets = decoder_targets.to(device)\n",
    "\n",
    "            outputs, _ = model(encoder_inputs, decoder_inputs)\n",
    "\n",
    "            loss = loss_function(outputs.view(-1, outputs.size(-1)), decoder_targets.view(-1))\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            mask = decoder_targets != 0\n",
    "            total_correct += ((outputs.argmax(dim=-1) == decoder_targets) * mask).sum().item()\n",
    "            total_count += mask.sum().item()\n",
    "\n",
    "    return total_loss / len(dataloader), total_correct / total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/59 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  3,  14,   6, 200,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0], device='mps:0')\n",
      "tensor([ 14,   6, 200,   2,   4,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0], device='mps:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for encoder_inputs, decoder_inputs, decoder_targets in tqdm(train_dataloader):\n",
    "    encoder_inputs = encoder_inputs.to(device)\n",
    "    decoder_inputs = decoder_inputs.to(device)\n",
    "    decoder_targets = decoder_targets.to(device)\n",
    "    print(decoder_inputs[0])\n",
    "    print(decoder_targets[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:09<00:00,  6.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/50 | Train Loss: 5.1982 | Train Acc: 0.4573 | Valid Loss: 5.6552 | Valid Acc: 0.4486\n",
      "Validation loss improved from inf to 5.6552. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:09<00:00,  6.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/50 | Train Loss: 3.3235 | Train Acc: 0.5239 | Valid Loss: 3.9800 | Valid Acc: 0.5081\n",
      "Validation loss improved from 5.6552 to 3.9800. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:09<00:00,  6.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/50 | Train Loss: 2.6583 | Train Acc: 0.5593 | Valid Loss: 3.3764 | Valid Acc: 0.5390\n",
      "Validation loss improved from 3.9800 to 3.3764. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:09<00:00,  6.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/50 | Train Loss: 2.2992 | Train Acc: 0.5785 | Valid Loss: 3.0723 | Valid Acc: 0.5510\n",
      "Validation loss improved from 3.3764 to 3.0723. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:09<00:00,  6.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/50 | Train Loss: 2.0457 | Train Acc: 0.5979 | Valid Loss: 2.8548 | Valid Acc: 0.5685\n",
      "Validation loss improved from 3.0723 to 2.8548. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:08<00:00,  6.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/50 | Train Loss: 1.7998 | Train Acc: 0.6292 | Valid Loss: 2.6705 | Valid Acc: 0.5949\n",
      "Validation loss improved from 2.8548 to 2.6705. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:08<00:00,  6.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/50 | Train Loss: 1.6410 | Train Acc: 0.6467 | Valid Loss: 2.5663 | Valid Acc: 0.6004\n",
      "Validation loss improved from 2.6705 to 2.5663. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:09<00:00,  6.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/50 | Train Loss: 1.4749 | Train Acc: 0.6698 | Valid Loss: 2.4467 | Valid Acc: 0.6152\n",
      "Validation loss improved from 2.5663 to 2.4467. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:08<00:00,  6.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/50 | Train Loss: 1.3340 | Train Acc: 0.6904 | Valid Loss: 2.3471 | Valid Acc: 0.6240\n",
      "Validation loss improved from 2.4467 to 2.3471. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:08<00:00,  6.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/50 | Train Loss: 1.2071 | Train Acc: 0.7127 | Valid Loss: 2.2630 | Valid Acc: 0.6357\n",
      "Validation loss improved from 2.3471 to 2.2630. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:08<00:00,  6.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11/50 | Train Loss: 1.1119 | Train Acc: 0.7213 | Valid Loss: 2.2156 | Valid Acc: 0.6363\n",
      "Validation loss improved from 2.2630 to 2.2156. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:08<00:00,  6.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12/50 | Train Loss: 1.0813 | Train Acc: 0.7302 | Valid Loss: 2.2142 | Valid Acc: 0.6331\n",
      "Validation loss improved from 2.2156 to 2.2142. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:09<00:00,  6.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13/50 | Train Loss: 0.9361 | Train Acc: 0.7536 | Valid Loss: 2.1092 | Valid Acc: 0.6585\n",
      "Validation loss improved from 2.2142 to 2.1092. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:09<00:00,  6.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14/50 | Train Loss: 0.8828 | Train Acc: 0.7677 | Valid Loss: 2.0738 | Valid Acc: 0.6624\n",
      "Validation loss improved from 2.1092 to 2.0738. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:09<00:00,  6.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15/50 | Train Loss: 0.8178 | Train Acc: 0.7788 | Valid Loss: 2.0516 | Valid Acc: 0.6707\n",
      "Validation loss improved from 2.0738 to 2.0516. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:08<00:00,  6.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16/50 | Train Loss: 0.7772 | Train Acc: 0.7854 | Valid Loss: 2.0425 | Valid Acc: 0.6719\n",
      "Validation loss improved from 2.0516 to 2.0425. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:08<00:00,  6.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17/50 | Train Loss: 0.7496 | Train Acc: 0.7909 | Valid Loss: 2.0282 | Valid Acc: 0.6727\n",
      "Validation loss improved from 2.0425 to 2.0282. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:08<00:00,  6.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18/50 | Train Loss: 0.6984 | Train Acc: 0.8005 | Valid Loss: 1.9978 | Valid Acc: 0.6776\n",
      "Validation loss improved from 2.0282 to 1.9978. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:08<00:00,  6.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19/50 | Train Loss: 0.6755 | Train Acc: 0.8052 | Valid Loss: 1.9939 | Valid Acc: 0.6824\n",
      "Validation loss improved from 1.9978 to 1.9939. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:08<00:00,  6.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20/50 | Train Loss: 0.6696 | Train Acc: 0.8092 | Valid Loss: 1.9927 | Valid Acc: 0.6840\n",
      "Validation loss improved from 1.9939 to 1.9927. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:08<00:00,  6.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21/50 | Train Loss: 0.6279 | Train Acc: 0.8155 | Valid Loss: 1.9763 | Valid Acc: 0.6855\n",
      "Validation loss improved from 1.9927 to 1.9763. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:08<00:00,  6.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22/50 | Train Loss: 0.6253 | Train Acc: 0.8125 | Valid Loss: 1.9713 | Valid Acc: 0.6806\n",
      "Validation loss improved from 1.9763 to 1.9713. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:08<00:00,  6.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23/50 | Train Loss: 0.6030 | Train Acc: 0.8199 | Valid Loss: 1.9713 | Valid Acc: 0.6908\n",
      "Validation loss improved from 1.9713 to 1.9713. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:08<00:00,  6.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24/50 | Train Loss: 0.5776 | Train Acc: 0.8247 | Valid Loss: 1.9738 | Valid Acc: 0.6961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:08<00:00,  6.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25/50 | Train Loss: 0.5660 | Train Acc: 0.8313 | Valid Loss: 1.9419 | Valid Acc: 0.6978\n",
      "Validation loss improved from 1.9713 to 1.9419. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:08<00:00,  6.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26/50 | Train Loss: 0.5496 | Train Acc: 0.8227 | Valid Loss: 1.9507 | Valid Acc: 0.6897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:08<00:00,  6.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27/50 | Train Loss: 0.5192 | Train Acc: 0.8399 | Valid Loss: 1.9377 | Valid Acc: 0.7016\n",
      "Validation loss improved from 1.9419 to 1.9377. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:08<00:00,  7.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28/50 | Train Loss: 0.5009 | Train Acc: 0.8432 | Valid Loss: 1.9283 | Valid Acc: 0.7020\n",
      "Validation loss improved from 1.9377 to 1.9283. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:08<00:00,  6.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29/50 | Train Loss: 0.4956 | Train Acc: 0.8450 | Valid Loss: 1.9194 | Valid Acc: 0.7047\n",
      "Validation loss improved from 1.9283 to 1.9194. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:09<00:00,  6.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30/50 | Train Loss: 0.4993 | Train Acc: 0.8441 | Valid Loss: 1.9236 | Valid Acc: 0.7052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:09<00:00,  6.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31/50 | Train Loss: 0.4804 | Train Acc: 0.8497 | Valid Loss: 1.9213 | Valid Acc: 0.7074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:09<00:00,  6.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32/50 | Train Loss: 0.4722 | Train Acc: 0.8500 | Valid Loss: 1.9063 | Valid Acc: 0.7089\n",
      "Validation loss improved from 1.9194 to 1.9063. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:08<00:00,  6.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 33/50 | Train Loss: 0.4514 | Train Acc: 0.8557 | Valid Loss: 1.9059 | Valid Acc: 0.7089\n",
      "Validation loss improved from 1.9063 to 1.9059. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:08<00:00,  6.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34/50 | Train Loss: 0.4460 | Train Acc: 0.8583 | Valid Loss: 1.9147 | Valid Acc: 0.7052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:08<00:00,  7.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 35/50 | Train Loss: 0.4322 | Train Acc: 0.8610 | Valid Loss: 1.8986 | Valid Acc: 0.7119\n",
      "Validation loss improved from 1.9059 to 1.8986. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:08<00:00,  6.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36/50 | Train Loss: 0.4260 | Train Acc: 0.8614 | Valid Loss: 1.9150 | Valid Acc: 0.7121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:08<00:00,  6.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 37/50 | Train Loss: 0.4007 | Train Acc: 0.8676 | Valid Loss: 1.9004 | Valid Acc: 0.7146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:08<00:00,  6.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 38/50 | Train Loss: 0.4185 | Train Acc: 0.8642 | Valid Loss: 1.9186 | Valid Acc: 0.7120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:08<00:00,  6.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39/50 | Train Loss: 0.3955 | Train Acc: 0.8694 | Valid Loss: 1.8961 | Valid Acc: 0.7133\n",
      "Validation loss improved from 1.8986 to 1.8961. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:08<00:00,  6.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 40/50 | Train Loss: 0.4116 | Train Acc: 0.8634 | Valid Loss: 1.9275 | Valid Acc: 0.7024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:08<00:00,  6.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 41/50 | Train Loss: 0.3908 | Train Acc: 0.8690 | Valid Loss: 1.9102 | Valid Acc: 0.7139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:08<00:00,  6.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 42/50 | Train Loss: 0.3865 | Train Acc: 0.8712 | Valid Loss: 1.9228 | Valid Acc: 0.7138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:08<00:00,  6.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 43/50 | Train Loss: 0.3775 | Train Acc: 0.8744 | Valid Loss: 1.8944 | Valid Acc: 0.7168\n",
      "Validation loss improved from 1.8961 to 1.8944. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:08<00:00,  6.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 44/50 | Train Loss: 0.3697 | Train Acc: 0.8740 | Valid Loss: 1.9021 | Valid Acc: 0.7150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:08<00:00,  6.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 45/50 | Train Loss: 0.3528 | Train Acc: 0.8772 | Valid Loss: 1.8913 | Valid Acc: 0.7196\n",
      "Validation loss improved from 1.8944 to 1.8913. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:08<00:00,  6.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 46/50 | Train Loss: 0.3398 | Train Acc: 0.8816 | Valid Loss: 1.8814 | Valid Acc: 0.7201\n",
      "Validation loss improved from 1.8913 to 1.8814. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:08<00:00,  6.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 47/50 | Train Loss: 0.3399 | Train Acc: 0.8835 | Valid Loss: 1.8869 | Valid Acc: 0.7205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:08<00:00,  6.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 48/50 | Train Loss: 0.3334 | Train Acc: 0.8855 | Valid Loss: 1.8887 | Valid Acc: 0.7210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:08<00:00,  6.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 49/50 | Train Loss: 0.3265 | Train Acc: 0.8857 | Valid Loss: 1.8837 | Valid Acc: 0.7206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:08<00:00,  6.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50/50 | Train Loss: 0.3044 | Train Acc: 0.8922 | Valid Loss: 1.8626 | Valid Acc: 0.7276\n",
      "Validation loss improved from 1.8814 to 1.8626. 체크포인트를 저장합니다.\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    for encoder_inputs, decoder_inputs, decoder_targets in tqdm(train_dataloader):\n",
    "        encoder_inputs = encoder_inputs.to(device)\n",
    "        decoder_inputs = decoder_inputs.to(device)\n",
    "        decoder_targets = decoder_targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs, _ = model(encoder_inputs, decoder_inputs)\n",
    "\n",
    "        loss = loss_function(outputs.view(-1, outputs.size(-1)), decoder_targets.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss, train_acc = evaluation(model, train_dataloader, loss_function, device)\n",
    "    valid_loss, valid_acc = evaluation(model, valid_dataloader, loss_function, device)\n",
    "\n",
    "    print(f'Epoch: {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Valid Loss: {valid_loss:.4f} | Valid Acc: {valid_acc:.4f}')\n",
    "\n",
    "    if valid_loss < best_val_loss:\n",
    "        print(f'Validation loss improved from {best_val_loss:.4f} to {valid_loss:.4f}. 체크포인트를 저장합니다.')\n",
    "        best_val_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'best_model_checkpoint.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model validation loss: 1.8626\n",
      "Best model validation accuracy: 0.7276\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('best_model_checkpoint.pth', weights_only=True))\n",
    "model.to(device)\n",
    "\n",
    "val_loss, val_accuracy = evaluation(model, valid_dataloader, loss_function, device)\n",
    "\n",
    "print(f'Best model validation loss: {val_loss:.4f}')\n",
    "print(f'Best model validation accuracy: {val_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_to_src(input_seq):\n",
    "  sentence = ''\n",
    "  for encoded_word in input_seq:\n",
    "    if(encoded_word != 0):\n",
    "      sentence = sentence + index2src[encoded_word] + ' '\n",
    "  return sentence\n",
    "\n",
    "def seq_to_tar(input_seq):\n",
    "  sentence = ''\n",
    "  for encoded_word in input_seq:\n",
    "    if(encoded_word != 0 and encoded_word != tgt_vocab['<sos>'] and encoded_word != tgt_vocab['<eos>']):\n",
    "      sentence = sentence + index2tar[encoded_word] + ' '\n",
    "  return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq, model):\n",
    "    model.eval()\n",
    "    encoder_inputs = torch.LongTensor(input_seq).unsqueeze(0).to(device)\n",
    "    src_mask = model.make_src_mask(encoder_inputs)\n",
    "    encoder_out = model.encode(encoder_inputs, src_mask)\n",
    "\n",
    "    decoded_tokens = [tgt_vocab['<sos>']]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_len):\n",
    "            decoder_input = torch.LongTensor(decoded_tokens).unsqueeze(0).to(device)\n",
    "            tgt_mask = model.make_tgt_mask(decoder_input)\n",
    "            src_tgt_mask = model.make_src_tgt_mask(encoder_inputs, decoder_input)\n",
    "            output = model.decode(decoder_input, encoder_out, tgt_mask, src_tgt_mask)\n",
    "            output = model.generator(output)\n",
    "\n",
    "            output_token = output.argmax(dim=2)[:, -1].item()\n",
    "\n",
    "            if output_token == tgt_vocab['<eos>']:\n",
    "                break\n",
    "\n",
    "            decoded_tokens.append(output_token)\n",
    "\n",
    "    return ' '.join(index2tar[token] for token in decoded_tokens if token != tgt_vocab['<sos>'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력문장:  keep back . \n",
      "정답문장:  restez en retrait . \n",
      "번역문장:  restez en arriere .\n",
      "--------------------------------------------------\n",
      "입력문장:  have you finished ? \n",
      "정답문장:  avez vous fini ? \n",
      "번역문장:  avez vous termine ?\n",
      "--------------------------------------------------\n",
      "입력문장:  please sing . \n",
      "정답문장:  s il vous plait chantez ! \n",
      "번역문장:  s il vous plait chantez !\n",
      "--------------------------------------------------\n",
      "입력문장:  are you surprised ? \n",
      "정답문장:  es tu surprise ? \n",
      "번역문장:  es tu surpris ?\n",
      "--------------------------------------------------\n",
      "입력문장:  quit hassling me . \n",
      "정답문장:  arretez de m embeter ! \n",
      "번역문장:  arrete de m embeter !\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for seq_index in [3, 50, 100, 300, 1001]:\n",
    "    input_seq = encoder_input_train[seq_index]\n",
    "    translated_text = decode_sequence(input_seq, model)\n",
    "\n",
    "    print(\"입력문장: \", seq_to_src(encoder_input_train[seq_index]))\n",
    "    print(\"정답문장: \", seq_to_tar(decoder_input_train[seq_index]))\n",
    "    print(\"번역문장: \", translated_text)\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recsys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
